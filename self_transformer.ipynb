{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**This project is an attempt at creating a transformer from scratch using the Multi Head Attention Mechanism which computes the attention between each pair of positions in a sequence. Consisting of multiple 'attention heads', that capture different aspects of the input sequence. This is a step ahead of simply using position within text to decipher meaning and importance.\n",
        "Transformers alleviate common issues in RNN's such as weaknesses in long range dependencies, gradient vanishing or gradient explosion problems (that either suddenly increase loss or stops the decrease in loss of the model) and more.**"
      ],
      "metadata": {
        "id": "-f0uRu0bypc3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P0gqUPv3arz6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "znGb3MV1TMVl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "    self.d_model = d_model\n",
        "    self.num_head = num_heads\n",
        "    self.head_dim = d_model // num_heads\n",
        "\n",
        "    # linear transformations for queries, keys and values for all heads\n",
        "    self.wq = nn.Linear(d_model, d_model)\n",
        "    self.wk = nn.Linear(d_model, d_model)\n",
        "    self.wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # linear transformation for the concatenated outputs of all heads\n",
        "    self.fc_out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, query, key, value, mask):\n",
        "        \"\"\"Calculate scaled dot-product attention for one head.\"\"\"\n",
        "\n",
        "        # calculate the dot product of the query and key and transpose the result\n",
        "        matmul_qk = torch.matmul(query, key.transpose(-2, -1))\n",
        "\n",
        "        # ge the dimension of the key vectors (d_k) which is the size of the last dimension of the query tensor\n",
        "        d_k = query.size(-1)\n",
        "\n",
        "        # scale the dot product by the square root of d_k to stabilize gradients\n",
        "        scaled_attention_logits = matmul_qk / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "\n",
        "\n",
        "        # apply a mask to the scaled_atention_logits if a mask is provided\n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += (mask * -1e9)  # Masked values have a large negative value\n",
        "\n",
        "        # compute the attention weights by applying the softmax function along the last dimension\n",
        "        attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)\n",
        "\n",
        "        # calculcate the weighted sum of the values using the attention weights\n",
        "        output = torch.matmul(attention_weights, value)\n",
        "\n",
        "        # return the output and attention weights\n",
        "        return output, attention_weights\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "      \"\"\"Split the input tensor into separate attention heads\"\"\"\n",
        "\n",
        "      # reshape the input tensor 'x' into shape that separates the attention heads\n",
        "      # reshape it into a 4D tensor with dimensions: (batch_size, num_heads, sequence_length, head_dim)\n",
        "\n",
        "      x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
        "\n",
        "      # permute the dimensions of the tensor to obtain the desired shape\n",
        "      # the result will have dimensions: (batch_size, num_heads, sequence_length, head_dim)\n",
        "\n",
        "      x = x.permute(0, 2, 1, 3)\n",
        "\n",
        "      # return reshaped tensor\n",
        "      return x\n",
        "\n",
        "    def combine_heads(self, x, batch_size):\n",
        "        \"\"\"Combine the attention heads back to the original shape\"\"\"\n",
        "\n",
        "        # uses 'permute' to rearrange dimensions and swaps dimensions to match original shape\n",
        "        # uses contiguous to ensure the tensor's memory layout is continguous in memory\n",
        "        # 'view' to reshape the tensor into desired shape with dimensions (batch+_size, sequence_length, d_model)\n",
        "        x = x.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return x\n",
        "\n",
        "    def forward(self, query, key, value, mask):\n",
        "        batch_size = query.size(0) #get the batch size from the query tensor\n",
        "\n",
        "        #linearly project queries, keys, and values for all heads\n",
        "        query = self.wq(query) #project queries using the wq linear year\n",
        "        key = self.wk(key) #project keys using the wk linear layer\n",
        "        value = self.wv(value) #project values using the wv linear layer\n",
        "\n",
        "        #split the queries, keys, and values into separate heads\n",
        "        query = self.split_heads(query, batch_size) #split queries into multiple heads\n",
        "        key = self.split_heads(key, batch_size) #split keys into multiple heads\n",
        "        value = self.split_heads(value, batch_size) #split values into multiple heads\n",
        "\n",
        "        #calculate scaled dot-product attention for each head\n",
        "        output, attention_weights = self.scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "        #combine the attention heads\n",
        "        output = self.combine_heads(output, batch_size)\n",
        "\n",
        "        #apply a linear transformation to the combined outputs\n",
        "        output = self.fc_out(output)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WwCJxrWeCLIN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_len=512):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.d_model = d_model # dimensionality of the positional encoding\n",
        "    self.max_len = max_len # maximum sequence length for which positional encoding will be generated\n",
        "\n",
        "    # create positional encoding matrix\n",
        "    pe = self.create_positional_encoding(max_len, d_model)\n",
        "\n",
        "    # register the positional encoding as a bugger(not a learnable parameter)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def create_positional_encoding(self, max_len, d_model):\n",
        "\n",
        "    #create a sequence of positions from 0 to max_len - 1\n",
        "    position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    # calculate the div_term for the positional encoding\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float()*(-math.log(10000.0)/d_model))\n",
        "\n",
        "    # intialize the positional encoding matrix\n",
        "    pe = torch.zeroes(max_len, d_model)\n",
        "\n",
        "    #compute sine and cosine values for positional encoding\n",
        "    pe[:, 0::2] = torch.sin(position*div_term) #selects every second element along the second dimension of the positional encoding of 'pe' tensor\n",
        "    pe[:, 1::2] = torch.cose(position*div_term) # then calculates the sin or cosine of '(position * div_term)' for each position in the sequence\n",
        "    # sin based encoding for even indices along the second dimension - sin encoded values\n",
        "    # cosine based encoding for odd indices along the second dimension of the 'pe' tensor - cosine encoded values\n",
        "\n",
        "    #add a batch dimension to the positional encoding matrix\n",
        "    pe = pe.unsqueeze(0)\n",
        "    return pe\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Add the positional encoding to the input embeddings\n",
        "    x = x + self.pe[:, :x.size(1)]\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "fmFdsU3pNFBd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff):\n",
        "    super(PositionWiseFeedForward, self).__init__()\n",
        "\n",
        "    #define the first fully connected layer with input size d_model and output size d_ff\n",
        "    self.fdc1 = nn.Linear(d_model, d_ff)\n",
        "\n",
        "    #define the second fully connected layer with input size d_ff and output size d_model\n",
        "    self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    #define the activation function ReLU (Rectified Linear Unit)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    #apply the first fully connected layer followed by ReLU activation\n",
        "    out1 = self.fc1(x)\n",
        "    out1 = self.relu(out1)\n",
        "\n",
        "    #apply the second fully connected layer to the output of the first\n",
        "    out2 = self.fc2(out1)\n",
        "\n",
        "    #return the output of the second layer\n",
        "    return out2\n",
        "\n"
      ],
      "metadata": {
        "id": "4JsEvLFBeWiY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "    \"\"\"\n",
        "    Initialize a single encoder layer in the transformer model\n",
        "\n",
        "    Arguments -\n",
        "    - d_model: the dimensionality of the model's hidden states\n",
        "    - num_heads: the number of attention heads of multi-head attention\n",
        "    - d_ff: the dimensionality of the feedforward sublayer\n",
        "    - dropout: the dropout rate to be applied within the layer\n",
        "\n",
        "    \"\"\"\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    #multi head self attention layer\n",
        "    self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "\n",
        "    #position wise feed forward layer\n",
        "    self.feedforward = PositionWiseFeedForward(d_model, d_ff)\n",
        "\n",
        "    #layer normalization\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    #dropout\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    #multi head self attention\n",
        "    attention_output, _=self.self_attention(x,x,x, mask=None)\n",
        "    x = x + self.dropout(attention_output)\n",
        "    x = self.norm1(x)\n",
        "\n",
        "    #position wise feedforward\n",
        "    feedforward_output = self.feedforward(x)\n",
        "    x = x + self.dropout(feedforward_output)\n",
        "    x = self.norm2(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "AsADoLmH0Vpy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, num_heads, dropout):  #source sequence mask and target sequence mask\n",
        "    \"\"\"\n",
        "    Initialize a single decoder layer in the transformer model\n",
        "\n",
        "    Arguments -\n",
        "    - d_model: the dimensionality of the model's hidden states\n",
        "    - num_heads: the number of attention heads of multi-head attention\n",
        "    - d_ff: the dimensionality of the feedforward sublayer\n",
        "    - dropout: the dropout rate to be applied within the layer\n",
        "\n",
        "    \"\"\"\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    #multi head self attention layer\n",
        "    self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    #multi head cross attention layer (for encoder decoder attention)\n",
        "    self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    #position wise feedforward\n",
        "    self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "\n",
        "    #layer normalization\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "    # dropout layer\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "      \"\"\"\n",
        "      Forward pass through the decoder layer.\n",
        "\n",
        "      argument:\n",
        "      - x: The input tensor representing the target sequence.\n",
        "      - enc_output: The output tensor from the encoder (encoder-decoder attention).\n",
        "      - src_mask: The source sequence mask (for encoder-decoder attention) used when decoder is attenting to encoder's outputs during decoing process\n",
        "                  Masks out padded values in source sequences\n",
        "      - tgt_mask: The target sequence mask (for self-attention) used when the decoder is attenting to its own targetsequence\n",
        "                  Ensures that each position in the target seq can only attend to positions before itself and not to positions after it\n",
        "\n",
        "      returns:\n",
        "      - x: The output tensor of the decoder layer.\n",
        "      \"\"\"\n",
        "      # Self-Attention Layer\n",
        "      self_attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "      x = x + self.dropout(self_attn_output)\n",
        "      x = self.norm1(x)\n",
        "\n",
        "      # Encoder-Decoder Attention Layer\n",
        "      cross_attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "      x = x + self.dropout(cross_attn_output)\n",
        "      x = self.norm2(x)\n",
        "\n",
        "      # Position-wise FeedForward Layer\n",
        "      ff_output = self.feed_forward(x)\n",
        "      x = x + self.dropout(ff_output)\n",
        "      x = self.norm3(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZILOIcO2RFpI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, num_layers, d_model, num_heads, d_ff, dropout):\n",
        "    \"\"\"\n",
        "    Initialize the Transformer encoder by stacking multiple encoder layers\n",
        "\n",
        "    arguments -\n",
        "    - num_layers: number of encoder layers to stack\n",
        "    - d_model: the dimensionality of the model's hidden states\n",
        "    - num_heads: the number of attention heads for multi-head attention\n",
        "    - d_ff: dimnsionality of feedforward sublayer\n",
        "    - dropout: the dropout rate to apply within the encoder layers\n",
        "\n",
        "    \"\"\"\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "\n",
        "    #stack multiple encoder layers\n",
        "    self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "  def forward(self, x, src_mask):\n",
        "    \"\"\"\n",
        "    Forward Pass through the transformer code\n",
        "\n",
        "    arguments:\n",
        "    - x: the input tensor representing the source sequence\n",
        "    - src_mark: the source sequence mask\n",
        "\n",
        "    returns:\n",
        "    - enc_output: the output tensor of the encoder\n",
        "    \"\"\"\n",
        "\n",
        "    enc_output = x\n",
        "\n",
        "    #iterate through encoder layers\n",
        "    for enc_layer in self.encoder_layers:\n",
        "      enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "\n",
        "    return enc_output"
      ],
      "metadata": {
        "id": "jYwJrZhta-0M"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "  def __init__(self, num_layers, d_model, num_heads, d_ff, dropout):\n",
        "    \"\"\" Initialize the Transformer decoder by stacking multiple decoder layers \"\"\"\n",
        "\n",
        "    super(TransformerDecoder, self).__init__()\n",
        "\n",
        "    #stack multiple decoder layers\n",
        "\n",
        "    self.decode_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout)for _ in range(num_layers)])\n",
        "\n",
        "  def forward(self, x, src_mask, enc_output, tgt_mmask):\n",
        "    \"\"\"forward pass through the transformer decoder\"\"\"\n",
        "\n",
        "    dec_output = x\n",
        "\n",
        "    #iterate through decoder layers\n",
        "\n",
        "    for dec_layers in self.decoder_layers:\n",
        "      dec_output = dec_layers(dec_output, enc_output, src_mask, tgt_mmask)\n",
        "    return dec_output\n"
      ],
      "metadata": {
        "id": "Al1ql2WlU589"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, encoder, decoder, src_pad_idx, tgt_pad_idx):\n",
        "    \"\"\" intialize the Transformer model with an encoder and decoder\n",
        "\n",
        "    arguments -\n",
        "    - encoder: the transformer encoder\n",
        "    - decoder: The Transformer decoder.\n",
        "    - src_pad_idx: the padding index for the source sequence (integer value representing the padding token index in the source sequence)\n",
        "                    - in nlp tasks sequences have variable lengths and padding is added to make all sequences in the batch to have the same length\n",
        "    - tgt_pad_idx: the padding index for the target sequence - in sequence to sequence tasks such as machine translation the target sequence also need to have same lengths\n",
        "                    - Padding tokens are added to the end of the shorter target sequence to ensure consistent batch processing\n",
        "    \"\"\"\n",
        "\n",
        "    super(Transformer, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.tgt_pad_idx = tgt_pad_idx\n",
        "  def forward(self, src, tgt):\n",
        "    \"forward pass through the Transformer model\"\n",
        "\n",
        "    src_mask = self.make_src_mask(src)\n",
        "    tgt_mask = self.make_tgt_mask(tgt, src)\n",
        "\n",
        "    enc_output = self.encoder(src, src_mask)\n",
        "    output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def make_src_mask(self, src):\n",
        "    # create a mask to indentify padding tokens in the source sequence\n",
        "    return (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "  def make_tgt_mask(self, tgt, src):\n",
        "    #create masks for both self attention and cross attention in the decoder\n",
        "    tgt_mask = (tgt != self.tgt_pad_idx).unsqueeze(2).unsqueeze(2)\n",
        "    tgt_mask = tgt_mask & self.subsequent_mask(tgt.size(-1))\n",
        "    src_mask = (src != self.src_pad_idx).unsqueeze(1).unqueeze(2)\n",
        "    return tgt_mask, src_mask\n",
        "\n",
        "  def subsequent_mask(self, size):\n",
        "    #create a mask to prevent attending to future tokens in self-attention\n",
        "    return torch.triu(torch.ones(1, size, size), diagonal=1).bool()\n"
      ],
      "metadata": {
        "id": "w8O-jy_MqnFG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note about Padding Tokens - During the training and inference phases, the Transformer model should not pay attention to padding tokens because they do not carry meaningful informatio. These padding indices are used to create masks that prevent the model's attention mechanisms (self attention and cross attention) from attending to the padding tokens.\n",
        "In the make_src_mask and make_tgt_mask methods of the above Transformer class, these indices are used to create binary masks that can indentify the positions of padding tokens, which are then applied to the attention mechanisms to ensure that the model doesn't attend to padding tokens when calculating attention weights**"
      ],
      "metadata": {
        "id": "JVbewaccUP27"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V1juDDXBPhiB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}